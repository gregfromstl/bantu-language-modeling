{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kwere Character-Level Language Iterative N-Gram Model\n",
    "\n",
    "For my from scratch implementation I originally tried to do a vanilla n-gram model, but ran into speed issues with anything beyond a history of 5-characters, which had very poor performance.\n",
    "\n",
    "Instead, I decided to try to implement a sort of \"Iterative\" N-Gram Model that works backwards to find the longest possible relatively frequent sequence. I explain more in the documentation below but the important thing to note is the model works *backwards* from the target character when computing probabilities.\n",
    "\n",
    "So for example if an input sequence is [1,0,1,2,1], it'll first count the number of times [1] occurs. If it's greater than `threshold`, it'll \"overflow\" and begin counting sequences of [X, 1]. \n",
    "\n",
    "It will recursively so this until a less common sequence is reached. So if [1, 2, 1] is a very common sequence, it'll make the next prediction based on [0, 1, 2, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "As you'll see below, I reached a cross entropy of 2.228 While this seems high, it's not too far off from my anything goes implementation, so either it's not bad or I messed that up as well, which is fairly likely. The accuracy was (what seems) a reasonable 60%, considering the lack of Kwere training data and the messiness of the Swahili data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Dictionary containing all parameters for ease of tuning. These will be logged to the neptune logger below.\n",
    "\n",
    "**To add test data, enter the test file name in the `test_data` parameter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'experiment_name': \"Kwere\",\n",
    "    'tags': [\"kwere\", \"from scratch\"],\n",
    "    'n': 1000,\n",
    "    'threshold': 750,\n",
    "    'train_iterations': 10,\n",
    "    'carry_hidden_state': False,\n",
    "    'val_split': 0.3,\n",
    "    'kwere_train': \"./cwe-train.txt\",\n",
    "    'pretrain_iterations': 5,\n",
    "    'pretrain_percentage': 0.01, \n",
    "    'swahili': \"./sw-train.txt\",\n",
    "    'test_data': \"./cwe-test.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only import. Used for the log function to compute cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class\n",
    "The `Dataset` class generates a list of all unique characters found in the supplied data, number of total characters, number of unique characters, mappings from characters to their respective ID, mappings from chracter IDs to characters for making outputs readable, and a data tensor of every character converted to its ID.\n",
    "\n",
    "The `Dataset` will also generate a `~` character to be used in place of any characters unknown to the model (i.e. anything not in the training set). See the `clean_data` function below.\n",
    "\n",
    "Inputs:\n",
    " - `raw_data`: `string` of all characters from the provided data in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, raw_data: str):\n",
    "        self.chars = set(list(set(raw_data)))\n",
    "        self.chars.add('~')\n",
    "        self.data_size, self.vocab_size = len(raw_data), len(self.chars)\n",
    "        print(\"{} characters, {} unique\".format(self.data_size, self.vocab_size))\n",
    "        \n",
    "        self.char_to_idx = { char: idx for idx, char in enumerate(self.chars) }\n",
    "        self.idx_to_char = { idx: char for idx, char in enumerate(self.chars) }\n",
    "        \n",
    "        self.data = [self.char_to_idx[char] for char in list(raw_data)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "The `clean_data` function removes any unknown chracters in the provided data and replaces them with the deisgnated unknown chracter of `~`. I'm essentially forfeiting these characters if they ever appear in the testing data, since I likely couldn't get them correct anyway considering the model did not see them during training (unless they appear in the Kwere data, but see my explanation below for that decision).\n",
    "\n",
    "Inputs:\n",
    " - `raw_data`: `string` of raw data read directly from file\n",
    " - `known_chars`: `list` of `string` to be included in the data. Everything not in this list will be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(raw_data: str, known_chars: str) -> str:\n",
    "    cleaned = \"\"\n",
    "    for char in raw_data:\n",
    "        if char not in known_chars:\n",
    "            cleaned += \"~\"\n",
    "        else:\n",
    "            cleaned += char\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Load the Swahili training data and split based on the provided ratio. Then load the percentage of the Kwere data requested (see `PARAMS`). Finally, if a test file is provided in `PARAMS`, load the test data.\n",
    "\n",
    "The validation, Kwere, and test data are all cleaned of unknown chracters. I chose to exclude any chracters found in the Swahili data but not found in the Swahili training data for the sake of staying as true to the Swahili language as possible (in the event Kwere uses a character that Kwere does not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Kwere training data:\n",
      "\t422402 characters, 32 unique\n",
      "Loading Kwere validation data:\n",
      "\t181030 characters, 32 unique\n",
      "Loading Swahili data:\n",
      "\t392610 characters, 32 unique\n",
      "Loading Testing data:\n",
      "\t61717 characters, 32 unique\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Kwere training data:\", end=\"\\n\\t\")\n",
    "raw_kwere = open(PARAMS['kwere_train'], 'r').read()\n",
    "kwere_train_size, kwere_val_size = int(len(raw_kwere)*(1-PARAMS['val_split'])), int(len(raw_kwere)*PARAMS['val_split'])\n",
    "\n",
    "train_data = Dataset(raw_kwere[:kwere_train_size])\n",
    "\n",
    "print(\"Loading Kwere validation data:\", end=\"\\n\\t\")\n",
    "cleaned_kwere_val_data = clean_data(raw_kwere[kwere_train_size:], train_data.chars)\n",
    "val_data = Dataset(cleaned_kwere_val_data)\n",
    "\n",
    "\n",
    "if PARAMS['pretrain_percentage'] > 0:\n",
    "    print(\"Loading Swahili data:\", end=\"\\n\\t\")\n",
    "    raw_swahili = open(PARAMS['swahili'], 'r').read()\n",
    "    swahili_size = int(len(raw_swahili) * PARAMS['pretrain_percentage'])\n",
    "\n",
    "    cleaned_swahili_data = clean_data(raw_swahili[:swahili_size], train_data.chars)\n",
    "    pretrain_data = Dataset(cleaned_swahili_data)\n",
    "\n",
    "\n",
    "if len(PARAMS['test_data']) > 0:\n",
    "    print(\"Loading Testing data:\", end=\"\\n\\t\")\n",
    "    raw_test = open(PARAMS['test_data'], 'r').read()\n",
    "\n",
    "    cleaned_test_data = clean_data(raw_test, train_data.chars)\n",
    "    test_data = Dataset(cleaned_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Matrix\n",
    "The count matrix acts as a \"node\" in a web of matrices. The `counts` variable represents the counts of each character following the path to the current count matrix. The `next` variable contains the count matrices further down the path if an overflow has occurred. Otherwise, it's merely a dictionary of `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountMatrix:\n",
    "    def __init__(self, vocab: list, init_matrix=None):\n",
    "        self.counts = init_matrix if init_matrix is not None else {i:0 for i in vocab}\n",
    "        self.next = {i:None for i in vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increment Count\n",
    "The `increment_count` function increases the counts of characters following sequences while also controlling for overflow.\n",
    "\n",
    "Overflow:\n",
    "If a sequence has occured `threshold` number of times, the algorithm begins tracking the sequence's children, i.e. the combination of all possible preceding characters followed by the current sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_count(char: str, sequence: list, count_matrix: CountMatrix) -> list:\n",
    "    next_char = sequence[-1]\n",
    "    \n",
    "    count_matrix.counts[char] += 1\n",
    "    if count_matrix.next[next_char] is not None:\n",
    "        count_matrix.next[next_char] = increment_count(char, sequence[:-1], count_matrix.next[next_char])\n",
    "    elif sum(count_matrix.counts.values()) > PARAMS['threshold']:\n",
    "        vocab = count_matrix.next.keys()\n",
    "        initial_matrix = {i:0 for i in vocab}\n",
    "        initial_matrix[char] += 1\n",
    "        count_matrix.next = {i:CountMatrix(vocab, initial_matrix) for i in vocab}\n",
    "    \n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iterate_counts` simply calls `increment_count` for each character in the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_counts(data: Dataset, n: int, count_matrix: CountMatrix):\n",
    "    for idx, char in enumerate(data[n:]):\n",
    "        idx = n + idx\n",
    "        sequence = data[idx-n:idx]\n",
    "        \n",
    "        count_matrix = increment_count(data[idx], sequence, count_matrix)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build matrix using the above functions. I iterate over each dataset multiple times to account for the ratio in how much each dataset is taken into account. Since the pretrain set is so large, I iterate over the train set more so the pretrain set's probabilities don't overshadow the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Matrix...\n",
      "Fitting...\n"
     ]
    }
   ],
   "source": [
    "print(\"Building Matrix...\")\n",
    "count_matrix = CountMatrix(vocab=train_data.idx_to_char.keys())\n",
    "\n",
    "print(\"Fitting...\")\n",
    "if PARAMS['pretrain_percentage'] > 0:\n",
    "    for i in range(PARAMS['pretrain_iterations']):\n",
    "        count_matrix = iterate_counts(pretrain_data, PARAMS['n'], count_matrix)\n",
    "\n",
    "for i in range(PARAMS['train_iterations']):\n",
    "    count_matrix = iterate_counts(train_data, PARAMS['n'], count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for computing probabilities based on a dictionary of counts. Uses +1 smoothing.\n",
    "\n",
    "Also contains an assertion to ensure all probabilities sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities_from_counts(counts: dict):\n",
    "    # add one smoothing\n",
    "    counts = {key:counts[key]+1 for key in counts.keys()}\n",
    "    \n",
    "    probabilities = {key: counts[key] / sum(counts.values()) for key in counts.keys()}\n",
    "    prob_sum = sum(probabilities.values())\n",
    "    assert(abs(prob_sum - 1) < 0.0001), \"Probabilities should sum to 1.0 but got {}\".format(prob_sum)\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Probabilities\n",
    "1. Start with a given sequence of N characters\n",
    "2. Go to the `CountMatrix` of the last letter in the sequence (immediately preceding the target character)\n",
    "3. If this CountMatrix has no further path (`count_matrix.next` is all `None`), then the longest tracked path has been reached so determine the most likely character from the current matrix\n",
    "4. If more paths are possible, make a recursive call to get more history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities_for_sequence(sequence: list, count_matrix: CountMatrix):\n",
    "    # return counts if sequence has been exhausted\n",
    "    if len(sequence) == 0:\n",
    "        return count_matrix.counts\n",
    "    \n",
    "    next_char = sequence[-1]\n",
    "    \n",
    "    if count_matrix.next[next_char] is not None:\n",
    "        return get_probabilities_for_sequence(sequence[:-1], count_matrix.next[next_char])\n",
    "    else:\n",
    "        return probabilities_from_counts(count_matrix.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(target_prob):\n",
    "    return -math.log(target_prob, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data: Dataset, n: int, count_matrix: CountMatrix):\n",
    "    print(\"Evaluating...\")\n",
    "    \n",
    "    counter = 0\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    \n",
    "    for idx, char in enumerate(data[n:]):\n",
    "        idx = n + idx\n",
    "        sequence = data[idx-n:idx]\n",
    "\n",
    "        probabilities: dict = get_probabilities_for_sequence(sequence, count_matrix)\n",
    "        pred: str = max(probabilities, key=probabilities.get)\n",
    "        target: str = data[idx]\n",
    "        target_prob: float = probabilities[target]\n",
    "        \n",
    "        running_loss += calc_loss(target_prob)\n",
    "        running_acc += 1 if target == pred else 0\n",
    "        counter += 1\n",
    "        \n",
    "    return running_loss / counter, running_acc / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Train Loss: 1.631\t\t|\tTrain Accuracy: 67.65%\n",
      "Evaluating...\n",
      "Validation Loss: 2.228\t\t|\tValidation Accuracy: 59.98%\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = eval(train_data, PARAMS['n'], count_matrix)\n",
    "print(\"Train Loss: {:.3f}\\t\\t|\\tTrain Accuracy: {:.2f}%\".format(train_loss, train_acc*100))\n",
    "\n",
    "val_loss, val_acc = eval(val_data, PARAMS['n'], count_matrix)\n",
    "print(\"Validation Loss: {:.3f}\\t\\t|\\tValidation Accuracy: {:.2f}%\".format(val_loss, val_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Test Loss: 2.228\t\t|\tTest Accuracy: 59.55%\n"
     ]
    }
   ],
   "source": [
    "if len(PARAMS['test_data']) > 0:\n",
    "    test_loss, test_acc = eval(test_data, PARAMS['n'], count_matrix)\n",
    "    print(\"Test Loss: {:.3f}\\t\\t|\\tTest Accuracy: {:.2f}%\".format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
